services:
  namenode:
    image: apache/hadoop:3.4.0
    container_name: namenode
    mem_limit: 1g
    hostname: namenode

    environment:
      - HADOOP_NAMENODE_OPTS=-Xms512m -Xmx1024m
    entrypoint: ["/bin/bash", "-lc", "exec hdfs namenode"]
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./hadoop_namenode:/hadoop/dfs/name
      - ./bashrc_namenode:/opt/hadoop/.bashrc
      - ./scripts/init-hdfs.sh:/usr/local/bin/init-hdfs.sh:ro
    ports:
      - "9870:9870" # NN WebUI
      - "9000:9000" # HDFS RPC
    ulimits:
      nofile: 65536
      nproc: 4096
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -fsS http://localhost:9870/jmx | head -n1 >/dev/null",
        ]
      interval: 10s
      timeout: 3s
      retries: 12

  secondarynamenode:
    image: apache/hadoop:3.4.0
    depends_on:
      namenode:
        condition: service_healthy
    hostname: secondarynamenode
    mem_limit: 1g
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./hadoop_snn:/hadoop/dfs/namesecondary

    environment:
      - HADOOP_NAMENODE_OPTS=-Xms512m -Xmx1024m
      # 先禁用 Hadoop 本地库，验证稳定性（确认无关后再删掉）
      - HADOOP_OPTS=-Dorg.apache.hadoop.util.NativeCodeLoader.disable=true
      # 降低 glibc arena，防止小内存容器碎片化（可留可删）
      - MALLOC_ARENA_MAX=2
    entrypoint:
      ["/bin/bash", "-lc", "ulimit -n 65536; exec hdfs secondarynamenode"]
    ports:
      - "9868:9868"
    ulimits:
      nofile: 65536
      nproc: 4096

  datanode1:
    image: apache/hadoop:3.4.0
    mem_limit: 1g
    depends_on:
      namenode:
        condition: service_healthy
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./hadoop_datanode1:/hadoop/dfs/data
    ports:
      - "9864:9864"
    ulimits:
      nofile: 65536
      nproc: 4096
    entrypoint: ["/bin/bash", "-lc"]
    command: ["exec hdfs datanode"]

  resourcemanager:
    image: apache/hadoop:3.4.0
    mem_limit: 1g
    container_name: resourcemanager
    hostname: resourcemanager
    depends_on:
      namenode:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-lc", "exec yarn resourcemanager"]
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
    ports:
      - "8088:8088" # YARN RM WebUI
    ulimits:
      nofile: 65536
      nproc: 4096

  nodemanager1:
    image: apache/hadoop:3.4.0
    mem_limit: 1g
    container_name: nodemanager1
    hostname: nodemanager1
    depends_on:
      resourcemanager:
        condition: service_started
    entrypoint: ["/bin/bash", "-lc", "exec yarn nodemanager"]
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
    ports:
      - "8042:8042" # NM WebUI
    ulimits:
      nofile: 65536
      nproc: 4096

  historyserver:
    image: apache/hadoop:3.4.0
    mem_limit: 1g
    container_name: historyserver
    hostname: historyserver
    depends_on:
      resourcemanager:
        condition: service_started
      namenode:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-lc", "exec mapred historyserver"]
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./tmp/hadoop-history:/tmp/hadoop-yarn/staging
    ports:
      - "19888:19888" # MR JobHistory WebUI
    ulimits:
      nofile: 65536
      nproc: 4096

  nn-init:
    image: apache/hadoop:3.4.0
    user: "1000:100" # 建议：与目录属主一致
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./hadoop_namenode:/hadoop/dfs/name
    entrypoint: [
        "/bin/bash",
        "-lc",
        "set -e; set -u; set -o pipefail; \
        if [ ! -f /hadoop/dfs/name/current/VERSION ]; then \
        echo '[nn-init] Formatting HDFS NameNode...'; \
        hdfs namenode -format -force -nonInteractive; \
        else \
        echo '[nn-init] NameNode already formatted.'; \
        fi",
      ]
  init-hdfs:
    image: apache/hadoop:3.4.0
    depends_on:
      namenode:
        condition: service_healthy
      datanode1:
        condition: service_started
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./scripts/init-hdfs.sh:/usr/local/bin/init-hdfs.sh:ro
    # environment:
    # 如需覆盖默认：HDFS_SPARK_USER/HDFS_SPARK_GROUP/HDFS_SPARK_LOG_DIR 等
    # HDFS_SPARK_USER: "spark"
    # SPARK_LOG_DIR_POLICY: "chown"   # 或 "public"
    entrypoint: ["/bin/bash", "-lc", "/usr/local/bin/init-hdfs.sh"]
  # init-hdfs:
  #   image: apache/hadoop:3.4.0
  #   depends_on:
  #     namenode:
  #       condition: service_healthy
  #     datanode1:
  #       condition: service_started
  #   volumes:
  #     - ./conf:/opt/hadoop/etc/hadoop
  #   entrypoint:
  #     # 直接把脚本写成一行，交给 bash -lc
  #     - /bin/bash
  #     - -lc
  #     - |
  #       set -Eeuo pipefail;
  #       until hdfs dfs -ls / >/dev/null 2>&1; do sleep 1; done;
  #       hdfs dfs -mkdir -p /tmp /user/hadoop /mr-history/tmp /mr-history/done /spark-logs;
  #       hdfs dfs -chmod 1777 /tmp /mr-history/tmp /mr-history/done;
  #       hdfs dfs -chmod 755 /user;
  #       hdfs dfs -chown -R hadoop:supergroup /user/hadoop /spark-logs;
  #       hdfs dfs -ls /
networks:
  default:
    external: true
    name: hadoopnet
