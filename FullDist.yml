services:
  master:
    image: flyingbucket/hadoop_spark:0.0.2
    container_name: master
    hostname: master
    volumes:
      - ./full_dist_conf:/opt/hadoop/etc/hadoop
      - ./full_dist_data/namenode:/hadoop/dfs/name
      - ./scripts:/scripts:ro
    ports:
      - "9870:9870" # NN WebUI
      - "9000:9000" # HDFS RPC
      - "8088:8088" # YARN RM WebUI
      - "7077:7077" # Spark Master RPC
      - "8080:8080" # Spark Master WebUI
    environment:
      - HADOOP_ROOT_LOGGER=INFO,console
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=hdfs://namenode:9000/spark-logs
    entrypoint: ["tini", "--", "/scripts/master_ep.sh"]
    ulimits:
      nofile: 65536
      nproc: 4096
    healthcheck:
      test:
        ["CMD-SHELL", "curl -fsS http://localhost:9870/ | head -n1 >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30

  worker1:
    image: flyingbucket/hadoop_spark:0.0.2
    container_name: worker1
    hostname: worker1
    depends_on:
      master:
        condition: service_healthy
    volumes:
      - ./full_dist_conf:/opt/hadoop/etc/hadoop
      - ./full_dist_data/datanode1:/hadoop/dfs/data
      - ./scripts:/scripts:ro
    ports:
      - "9864:9864" # DN WebUI
      - "8042:8042" # NM WebUI
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    entrypoint: ["tini", "--", "/scripts/worker_ep.sh"]

  worker2:
    image: flyingbucket/hadoop_spark:0.0.2
    container_name: worker2
    hostname: worker2
    depends_on:
      master:
        condition: service_healthy
    volumes:
      - ./full_dist_conf:/opt/hadoop/etc/hadoop
      - ./full_dist_data/datanode2:/hadoop/dfs/data
      - ./scripts:/scripts:ro
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    entrypoint: ["tini", "--", "/scripts/worker_ep.sh"]

  spark-history:
    image: flyingbucket/hadoop_spark:0.0.2
    container_name: spark-history
    hostname: spark-history
    depends_on:
      master:
        condition: service_healthy
    volumes:
      - ./full_dist_conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts:ro
    ports:
      - "18080:18080" # Spark History WebUI
    environment:
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=hdfs://namenode:9000/spark-logs
    entrypoint: ["tini", "--", "/scripts/history_ep.sh"]

networks:
  default:
    name: FullDistNet
